{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances, accuracy_score\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn_lvq import GlvqModel\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style('white')\n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipolarize(Y):\n",
    "    X = np.copy(Y)\n",
    "    X[X > 0] = 1.0\n",
    "    X[X < 0] = -1.0\n",
    "    X[X == 0] = np.random.choice([-1.0, 1.0], size=len(X[X == 0]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroids(X,label=None):\n",
    "    if label is not None:\n",
    "        cLabel = np.unique(label)\n",
    "        c = np.zeros((len(cLabel), X.shape[1]))\n",
    "        for i,l in enumerate(cLabel):\n",
    "            c[i,:] = bipolarize(np.sum(X[label==l],axis=0))\n",
    "    else:\n",
    "        c = bipolarize(np.sum(X,axis=0)).reshape(1,-1)\n",
    "        cLabel = [0]\n",
    "    return cLabel, c.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(v,am,metric):\n",
    "    d = cdist(v,am,metric)\n",
    "    label = np.argmin(d,axis=1)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prototypes(pr,hv,label,ax,isMain=False):\n",
    "    allDist = cdist(pr,hv,'hamming')\n",
    "    \n",
    "    correctDist = np.array([allDist[x,i] for i,x in enumerate(label)])\n",
    "    nextDist = [np.min(np.delete(x,label[i])) for i,x in enumerate(allDist.T)]\n",
    "    \n",
    "    margin = (nextDist - correctDist)/(correctDist + nextDist)\n",
    "    \n",
    "    if isMain:\n",
    "        sns.kdeplot(correctDist,ax=ax[0],color='k',linestyle=':',linewidth=5)\n",
    "        sns.kdeplot(margin,ax=ax[1],color='k',linestyle=':',linewidth=5)\n",
    "    else:\n",
    "        sns.kdeplot(correctDist,ax=ax[0],linewidth=3,alpha=0.6)\n",
    "        sns.kdeplot(margin,ax=ax[1],linewidth=3,alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LVQHDC(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, initialPrototypes=None, initialPrototypeLabels=None, FPalpha=0.1, FNalpha=0.1, FNdecay=0.99, FPdecay=0.99, maxIter=100, stopTol=0, verbose=False):\n",
    "        self._prototypes = initialPrototypes\n",
    "        self._prototypeLabels = initialPrototypeLabels\n",
    "        self._allPrototypes = None\n",
    "        self._FPalpha = FPalpha\n",
    "        self._FNalpha = FNalpha\n",
    "        self._FPdecay = FPdecay\n",
    "        self._FNdecay = FNdecay\n",
    "        self._maxIter = maxIter\n",
    "        self._stopTol = stopTol\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        self.trainAcc = None\n",
    "        self.classes = None\n",
    "        self.valAcc = None\n",
    "        \n",
    "        self.D = 0\n",
    "        \n",
    "    def fit(self, X, y, grouping=None, imDir=None):\n",
    "        if self._prototypes is None:\n",
    "            self._prototypeLabels, self._prototypes = centroids(X,label=y)\n",
    "            self.classes = np.unique(y)\n",
    "            self._allPrototypes = [[] for _ in range(len(self.classes))]\n",
    "            self.D = X.shape[1]\n",
    "            \n",
    "        self.trainAcc = []\n",
    "        for i in range(self._maxIter):\n",
    "            if imDir is not None:\n",
    "                f,ax = plt.subplots(1,2,figsize=(20,10))\n",
    "                for pos in np.unique(grouping):\n",
    "                     analyze_prototypes(self._prototypes,X[grouping == pos],y[grouping == pos],ax,isMain=False)\n",
    "                analyze_prototypes(self._prototypes,X,y,ax,isMain=True)\n",
    "                ax[0].set_xlabel('Hamming distance to correct prototype')\n",
    "                ax[1].set_xlabel('Hamming distance margin')\n",
    "                ax[0].set_ylabel('Probability density')\n",
    "                ax[1].set_ylabel('Probability density')\n",
    "                ax[0].set_xlim((0,0.55))\n",
    "                ax[1].set_xlim((-0.25,0.25))\n",
    "                ax[0].set_ylim((0,25))\n",
    "                ax[1].set_ylim((0,50))\n",
    "                plt.savefig(imDir + 'iter_' + str(i) + '.png')\n",
    "                plt.close(f)\n",
    "            \n",
    "            predictions = self._prototypeLabels[classify(X,self._prototypes,'hamming')]\n",
    "            self.trainAcc.append(accuracy_score(predictions,y))\n",
    "            if self._verbose:\n",
    "                print('Iteration %d model fitting accuracy: %f' % (i, self.trainAcc[-1]))\n",
    "                        \n",
    "            for g in self.classes:\n",
    "                self._allPrototypes[g].append(np.copy(self._prototypes[g]))\n",
    "                \n",
    "                FNidx = np.where((y == g) & (predictions != g))[0]\n",
    "                FPidx = np.where((y != g) & (predictions == g))[0]\n",
    "\n",
    "                FNvec = bipolarize(np.sum(X[FNidx],axis=0))\n",
    "                FPvec = bipolarize(np.sum(X[FPidx],axis=0))\n",
    "\n",
    "                FNbits = np.random.choice(self.D,round(self._FNalpha*self.D),replace=False)\n",
    "                FPbits = np.random.choice(self.D,round(self._FPalpha*self.D),replace=False)\n",
    "\n",
    "                temp = np.copy(self._prototypes[self._prototypeLabels == g][0])\n",
    "                temp[FNbits] += FNvec[FNbits]*2\n",
    "                temp[FPbits] -= FPvec[FPbits]*2\n",
    "                temp = bipolarize(temp)\n",
    "                self._prototypes[self._prototypeLabels == g] = np.copy(temp)\n",
    "                \n",
    "            self._FPalpha *= self._FPdecay\n",
    "            self._FNalpha *= self._FNdecay\n",
    "            \n",
    "            if i > 0:\n",
    "                if np.mean(np.diff(self.trainAcc[-10:])) < self._stopTol:\n",
    "                    if self._verbose:\n",
    "                        print('Stopping condition reached!')\n",
    "                        break\n",
    "            \n",
    "    def fit_val(self, X, y, XVal, yVal):\n",
    "        if self._prototypes is None:\n",
    "            self._prototypeLabels, self._prototypes = centroids(X,label=y)\n",
    "            self.classes = np.unique(y)\n",
    "            self._allPrototypes = [[] for _ in range(len(self.classes))]\n",
    "            self.D = X.shape[1]\n",
    "            \n",
    "        self.trainAcc = []\n",
    "        self.valAcc = []\n",
    "        for i in range(self._maxIter):\n",
    "            predictions = self._prototypeLabels[classify(X,self._prototypes,'hamming')]\n",
    "            self.trainAcc.append(accuracy_score(predictions,y))\n",
    "            if self._verbose:\n",
    "                print('Iteration %d model fitting accuracy: %f' % (i, self.trainAcc[-1]))\n",
    "            \n",
    "            valPred = self._prototypeLabels[classify(XVal,self._prototypes,'hamming')]\n",
    "            self.valAcc.append(accuracy_score(valPred,yVal))\n",
    "            if self._verbose:\n",
    "                print('Iteration %d model validation accuracy: %f' % (i, self.valAcc[-1]))\n",
    "                                    \n",
    "            for g in self.classes:\n",
    "                self._allPrototypes[g].append(np.copy(self._prototypes[g]))\n",
    "                \n",
    "                FNidx = np.where((y == g) & (predictions != g))[0]\n",
    "                FPidx = np.where((y != g) & (predictions == g))[0]\n",
    "\n",
    "                FNvec = bipolarize(np.sum(X[FNidx],axis=0))\n",
    "                FPvec = bipolarize(np.sum(X[FPidx],axis=0))\n",
    "\n",
    "                FNbits = np.random.choice(self.D,round(self._FNalpha*self.D),replace=False)\n",
    "                FPbits = np.random.choice(self.D,round(self._FPalpha*self.D),replace=False)\n",
    "\n",
    "                temp = np.copy(self._prototypes[self._prototypeLabels == g][0])\n",
    "                temp[FNbits] += FNvec[FNbits]*2\n",
    "                temp[FPbits] -= FPvec[FPbits]*2\n",
    "                temp = bipolarize(temp)\n",
    "                self._prototypes[self._prototypeLabels == g] = np.copy(temp)\n",
    "                \n",
    "            self._FPalpha *= self._FPdecay\n",
    "            self._FNalpha *= self._FNdecay\n",
    "            \n",
    "            if i > 0:\n",
    "                if np.mean(np.diff(self.trainAcc[-10:])) < self._stopTol:\n",
    "                    if self._verbose:\n",
    "                        print('Stopping condition reached!')\n",
    "                        break\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = self._prototypeLabels[classify(X,self._prototypes,'hamming')]\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predictions = self._prototypeLabels[classify(X,self._prototypes,'hamming')]\n",
    "        return accuracy_score(predictions,y)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dataset and encoding type\n",
    "dataName = 'allHV.npz'\n",
    "emgHVType =  'hvRel'\n",
    "\n",
    "allHV = np.load(dataName)\n",
    "\n",
    "# extract data and labels based on gesture, trial, and position\n",
    "hv = allHV[emgHVType]\n",
    "gestLabel = allHV['gestLabel']\n",
    "posLabel = allHV['posLabel']\n",
    "trialLabel = allHV['trialLabel']\n",
    "\n",
    "combGP, groupGP = np.unique(np.column_stack((gestLabel,posLabel)),axis=0,return_inverse=True)\n",
    "combGPT, groupGPT = np.unique(np.column_stack((gestLabel,posLabel,trialLabel)),axis=0,return_inverse=True)\n",
    "\n",
    "# get list of unique values for each label\n",
    "gestures = np.unique(gestLabel)\n",
    "positions = np.unique(posLabel)\n",
    "trials = np.unique(trialLabel)\n",
    "\n",
    "numGestures = len(gestures)\n",
    "numPositions = len(positions)\n",
    "numTrials = len(trials)\n",
    "\n",
    "# get data size info\n",
    "D = hv.shape[1] # hypervector dimension\n",
    "numHV = 80 # number of examples per trial\n",
    "\n",
    "# color palettes for plotting\n",
    "gPalette = sns.color_palette('tab20', numGestures)\n",
    "pPalette = sns.color_palette('tab20', numPositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numSplit = 20\n",
    "# skf = StratifiedKFold(n_splits=numSplit)\n",
    "\n",
    "# X = hv\n",
    "# y = gestLabel\n",
    "# c = posLabel\n",
    "# g = groupGP\n",
    "\n",
    "# maxIter = 100\n",
    "# splitIdx = 0\n",
    "\n",
    "# allAccTrain = np.full_like(np.zeros((maxIter,numGestures,numPositions,numSplit)), np.nan, dtype=np.double)\n",
    "# centChangeTrain = np.full_like(np.zeros((maxIter,numGestures,numSplit)), np.nan, dtype=np.double)\n",
    "# centPosChangeTrain = np.full_like(np.zeros((maxIter,numGestures,numPositions,numSplit)), np.nan, dtype=np.double)\n",
    "\n",
    "# allAcc = np.full_like(np.zeros((maxIter,numGestures,numPositions,numSplit)), np.nan, dtype=np.double)\n",
    "# centChange = np.full_like(np.zeros((maxIter,numGestures,numSplit)), np.nan, dtype=np.double)\n",
    "# centPosChange = np.full_like(np.zeros((maxIter,numGestures,numPositions,numSplit)), np.nan, dtype=np.double)\n",
    "\n",
    "# maxActualIter = 0\n",
    "\n",
    "# for trainIdx, testIdx in skf.split(X,g):\n",
    "#     print('Running iteration %d of %d...' % (splitIdx+1, numSplit))\n",
    "#     XTrain, XTest = X[trainIdx], X[testIdx]\n",
    "#     yTrain, yTest = y[trainIdx], y[testIdx]\n",
    "#     cTrain, cTest = c[trainIdx], c[testIdx]\n",
    "    \n",
    "#     m = LVQHDC(FPalpha=0.02,FNalpha=0.1,FPdecay=0.9,FNdecay=0.97,maxIter=maxIter,verbose=True)\n",
    "#     m.fit_val(XTrain, yTrain, XTest, yTest)\n",
    "    \n",
    "#     ap = m._allPrototypes\n",
    "#     numIter = len(ap[0])\n",
    "#     maxActualIter = max(numIter,maxActualIter)\n",
    "\n",
    "#     amIters = [[] for _ in range(numIter)]\n",
    "#     for n in range(numIter):\n",
    "#         for g in gestures:\n",
    "#             amIters[n].append(ap[g][n])\n",
    "\n",
    "#     for g in gestures:\n",
    "#         for n in range(numIter):\n",
    "#             pred = classify(XTrain[yTrain == g],np.vstack(amIters[n]),'hamming')\n",
    "#             cSub = cTrain[yTrain == g]\n",
    "#             for p in positions:\n",
    "#                 allAccTrain[n,g,p,splitIdx] = accuracy_score(pred[cSub == p],np.ones(len(pred))[cSub == p]*g)\n",
    "#             pred = classify(XTest[yTest == g],np.vstack(amIters[n]),'hamming')\n",
    "#             cSub = cTest[yTest == g]\n",
    "#             for p in positions:\n",
    "#                 allAcc[n,g,p,splitIdx] = accuracy_score(pred[cSub == p],np.ones(len(pred))[cSub == p]*g)\n",
    "\n",
    "        \n",
    "#         cent = centroids(XTrain,yTrain)[1][g].reshape(1,-1)\n",
    "#         centPos = centroids(XTrain[yTrain==g],cTrain[yTrain==g])[1]\n",
    "        \n",
    "#         centChangeTrain[:numIter,g,splitIdx] = cdist(cent, np.vstack(ap[g]), 'hamming')[0]\n",
    "#         centPosChangeTrain[:numIter,g,:,splitIdx] = cdist(centPos, np.vstack(ap[g]), 'hamming').T\n",
    "        \n",
    "#         cent = centroids(XTest,yTest)[1][g].reshape(1,-1)\n",
    "#         centPos = centroids(XTest[yTest==g],cTest[yTest==g])[1]\n",
    "        \n",
    "#         centChange[:numIter,g,splitIdx] = cdist(cent, np.vstack(ap[g]), 'hamming')[0]\n",
    "#         centPosChange[:numIter,g,:,splitIdx] = cdist(centPos, np.vstack(ap[g]), 'hamming').T\n",
    "    \n",
    "#     splitIdx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sns.set(font_scale=2)\n",
    "# sns.set_style('white')\n",
    "# sns.set_context('poster')\n",
    "\n",
    "# f,ax = plt.subplots(1,3,figsize=(35,10))\n",
    "\n",
    "# a = allAcc\n",
    "# ax[0].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1,2,3))+np.nanstd(a,axis=(1,2,3)),np.nanmean(a,axis=(1,2,3))-np.nanstd(a,axis=(1,2,3)),alpha=0.05)\n",
    "# ax[0].plot(np.nanmean(a,axis=(1,2,3)))\n",
    "# a = allAccTrain\n",
    "# ax[0].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1,2,3))+np.nanstd(a,axis=(1,2,3)),np.nanmean(a,axis=(1,2,3))-np.nanstd(a,axis=(1,2,3)),alpha=0.05)\n",
    "# ax[0].plot(np.nanmean(a,axis=(1,2,3)))\n",
    "\n",
    "# for p in positions:\n",
    "#     a = allAcc[:,:,p,:]\n",
    "#     ax[1].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1,2))+np.nanstd(a,axis=(1,2)),np.nanmean(a,axis=(1,2))-np.nanstd(a,axis=(1,2)),alpha=0.05)\n",
    "#     ax[1].plot(np.nanmean(a,axis=(1,2)))\n",
    "#     a = allAccTrain[:,:,p,:]\n",
    "#     ax[2].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1,2))+np.nanstd(a,axis=(1,2)),np.nanmean(a,axis=(1,2))-np.nanstd(a,axis=(1,2)),alpha=0.05)\n",
    "#     ax[2].plot(np.nanmean(a,axis=(1,2)))\n",
    "    \n",
    "# ax[0].set_ylim(0,1)\n",
    "# ax[1].set_ylim(0,1)\n",
    "# ax[2].set_ylim(0,1)\n",
    "\n",
    "# ax[0].set_xlim(0,maxActualIter)\n",
    "# ax[1].set_xlim(0,maxActualIter)\n",
    "# ax[2].set_xlim(0,maxActualIter)\n",
    "\n",
    "# ax[0].set_title('Overall accuracy')\n",
    "# ax[0].legend(['Validation', 'Training'],loc='lower right')\n",
    "# ax[0].set_xlabel('Iteration')\n",
    "# ax[0].set_ylabel('Classification Accuracy')\n",
    "\n",
    "# ax[1].set_title('Validation accuracy by position')\n",
    "# ax[1].legend(['Pos %d' % x for x in positions],loc='lower right')\n",
    "# ax[1].set_xlabel('Iteration')\n",
    "# ax[1].set_ylabel('Classification Accuracy')\n",
    "\n",
    "# ax[2].set_title('Training accuracy by position')\n",
    "# ax[2].legend(['Pos %d' % x for x in positions],loc='lower right')\n",
    "# ax[2].set_xlabel('Iteration')\n",
    "# ax[2].set_ylabel('Classification Accuracy')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sns.set(font_scale=2)\n",
    "# sns.set_style('white')\n",
    "# sns.set_context('poster')\n",
    "\n",
    "# for g in gestures:\n",
    "#     f,ax = plt.subplots(2,3,figsize=(35,24))\n",
    "#     ax = ax.flatten()\n",
    "\n",
    "#     a = allAcc[:,g,:,:]\n",
    "#     ax[0].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1,2))+np.nanstd(a,axis=(1,2)),np.nanmean(a,axis=(1,2))-np.nanstd(a,axis=(1,2)),alpha=0.05)\n",
    "#     ax[0].plot(np.nanmean(a,axis=(1,2)))\n",
    "#     a = allAccTrain[:,g,:,:]\n",
    "#     ax[0].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1,2))+np.nanstd(a,axis=(1,2)),np.nanmean(a,axis=(1,2))-np.nanstd(a,axis=(1,2)),alpha=0.05)\n",
    "#     ax[0].plot(np.nanmean(a,axis=(1,2)))\n",
    "\n",
    "#     for p in positions:\n",
    "#         a = allAcc[:,g,p,:]\n",
    "#         ax[1].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1))+np.nanstd(a,axis=(1)),np.nanmean(a,axis=(1))-np.nanstd(a,axis=(1)),alpha=0.05)\n",
    "#         ax[1].plot(np.nanmean(a,axis=(1)))\n",
    "#         a = allAccTrain[:,g,p,:]\n",
    "#         ax[2].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1))+np.nanstd(a,axis=(1)),np.nanmean(a,axis=(1))-np.nanstd(a,axis=(1)),alpha=0.05)\n",
    "#         ax[2].plot(np.nanmean(a,axis=(1)))\n",
    "\n",
    "#     ax[0].set_ylim(0,1)\n",
    "#     ax[1].set_ylim(0,1)\n",
    "#     ax[2].set_ylim(0,1)\n",
    "\n",
    "#     ax[0].set_xlim(0,maxActualIter)\n",
    "#     ax[1].set_xlim(0,maxActualIter)\n",
    "#     ax[2].set_xlim(0,maxActualIter)\n",
    "\n",
    "#     ax[0].set_title('Gesture %d accuracy' % g)\n",
    "#     ax[0].legend(['Validation', 'Training'],loc='lower right')\n",
    "#     ax[0].set_xlabel('Iteration')\n",
    "#     ax[0].set_ylabel('Classification Accuracy')\n",
    "\n",
    "#     ax[1].set_title('Gesture %d validation accuracy by position' % g)\n",
    "#     ax[1].legend(['Pos %d' % x for x in positions],loc='lower right')\n",
    "#     ax[1].set_xlabel('Iteration')\n",
    "#     ax[1].set_ylabel('Classification Accuracy')\n",
    "\n",
    "#     ax[2].set_title('Gesture %d training accuracy by position' % g)\n",
    "#     ax[2].legend(['Pos %d' % x for x in positions],loc='lower right')\n",
    "#     ax[2].set_xlabel('Iteration')\n",
    "#     ax[2].set_ylabel('Classification Accuracy')\n",
    "    \n",
    "#     a = centChange[:,g,:]\n",
    "#     ax[3].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1))+np.nanstd(a,axis=(1)),np.nanmean(a,axis=(1))-np.nanstd(a,axis=(1)),alpha=0.05)\n",
    "#     ax[3].plot(np.nanmean(a,axis=(1)))\n",
    "#     a = centChangeTrain[:,g,:]\n",
    "#     ax[3].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1))+np.nanstd(a,axis=(1)),np.nanmean(a,axis=(1))-np.nanstd(a,axis=(1)),alpha=0.05)\n",
    "#     ax[3].plot(np.nanmean(a,axis=(1)))\n",
    "\n",
    "#     for p in positions:\n",
    "#         a = centPosChange[:,g,p,:]\n",
    "#         ax[4].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1))+np.nanstd(a,axis=(1)),np.nanmean(a,axis=(1))-np.nanstd(a,axis=(1)),alpha=0.05)\n",
    "#         ax[4].plot(np.nanmean(a,axis=(1)))\n",
    "#         a = centPosChangeTrain[:,g,p,:]\n",
    "#         ax[5].fill_between(np.arange(maxIter),np.nanmean(a,axis=(1))+np.nanstd(a,axis=(1)),np.nanmean(a,axis=(1))-np.nanstd(a,axis=(1)),alpha=0.05)\n",
    "#         ax[5].plot(np.nanmean(a,axis=(1)))\n",
    "\n",
    "#     ax[3].set_ylim(0,0.5)\n",
    "#     ax[4].set_ylim(0,0.5)\n",
    "#     ax[5].set_ylim(0,0.5)\n",
    "\n",
    "#     ax[3].set_xlim(0,maxActualIter)\n",
    "#     ax[4].set_xlim(0,maxActualIter)\n",
    "#     ax[5].set_xlim(0,maxActualIter)\n",
    "\n",
    "#     ax[3].set_title('Gesture %d prototype shift\\nrelative to overall centroid' % g)\n",
    "#     ax[3].legend(['Validation', 'Training'],loc='lower right')\n",
    "#     ax[3].set_xlabel('Iteration')\n",
    "#     ax[3].set_ylabel('Hamming distance')\n",
    "\n",
    "#     ax[4].set_title('Gesture %d prototype shift\\nrelative to validation position centroids' % g)\n",
    "#     ax[4].legend(['Pos %d' % x for x in positions],loc='lower right')\n",
    "#     ax[4].set_xlabel('Iteration')\n",
    "#     ax[4].set_ylabel('Hamming distance')\n",
    "\n",
    "#     ax[5].set_title('Gesture %d prototype shift\\nrelative to training position centroids' % g)\n",
    "#     ax[5].legend(['Pos %d' % x for x in positions],loc='lower right')\n",
    "#     ax[5].set_xlabel('Iteration')\n",
    "#     ax[5].set_ylabel('Hamming distance')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 model fitting accuracy: 0.759816\n",
      "Iteration 1 model fitting accuracy: 0.775240\n",
      "Iteration 2 model fitting accuracy: 0.791667\n",
      "Iteration 3 model fitting accuracy: 0.807813\n",
      "Iteration 4 model fitting accuracy: 0.817067\n",
      "Iteration 5 model fitting accuracy: 0.824519\n",
      "Iteration 6 model fitting accuracy: 0.826963\n",
      "Iteration 7 model fitting accuracy: 0.839864\n",
      "Iteration 8 model fitting accuracy: 0.840144\n",
      "Iteration 9 model fitting accuracy: 0.862260\n",
      "Iteration 10 model fitting accuracy: 0.871354\n",
      "Iteration 11 model fitting accuracy: 0.879407\n",
      "Iteration 12 model fitting accuracy: 0.886418\n",
      "Iteration 13 model fitting accuracy: 0.886979\n",
      "Iteration 14 model fitting accuracy: 0.898718\n",
      "Iteration 15 model fitting accuracy: 0.904888\n",
      "Iteration 16 model fitting accuracy: 0.911218\n",
      "Iteration 17 model fitting accuracy: 0.915625\n",
      "Iteration 18 model fitting accuracy: 0.921554\n",
      "Iteration 19 model fitting accuracy: 0.926082\n",
      "Iteration 20 model fitting accuracy: 0.927604\n",
      "Iteration 21 model fitting accuracy: 0.931811\n",
      "Iteration 22 model fitting accuracy: 0.937821\n",
      "Iteration 23 model fitting accuracy: 0.938862\n",
      "Iteration 24 model fitting accuracy: 0.939623\n",
      "Iteration 25 model fitting accuracy: 0.944351\n",
      "Iteration 26 model fitting accuracy: 0.947396\n",
      "Iteration 27 model fitting accuracy: 0.951322\n",
      "Iteration 28 model fitting accuracy: 0.950120\n",
      "Iteration 29 model fitting accuracy: 0.951522\n",
      "Iteration 30 model fitting accuracy: 0.950962\n",
      "Iteration 31 model fitting accuracy: 0.957652\n",
      "Iteration 32 model fitting accuracy: 0.957212\n",
      "Iteration 33 model fitting accuracy: 0.952845\n",
      "Iteration 34 model fitting accuracy: 0.954968\n",
      "Iteration 35 model fitting accuracy: 0.958333\n",
      "Iteration 36 model fitting accuracy: 0.954607\n",
      "Iteration 37 model fitting accuracy: 0.956410\n",
      "Iteration 38 model fitting accuracy: 0.960056\n",
      "Iteration 39 model fitting accuracy: 0.960938\n",
      "Iteration 40 model fitting accuracy: 0.953646\n",
      "Stopping condition reached!\n"
     ]
    }
   ],
   "source": [
    "X = hv\n",
    "y = gestLabel\n",
    "g = posLabel\n",
    "maxIter = 100\n",
    "m = LVQHDC(FPalpha=0,FNalpha=0.1,FPdecay=0.9,FNdecay=0.97,maxIter=maxIter,verbose=True)\n",
    "m.fit(X,y,grouping=g,imDir='./test/')\n",
    "\n",
    "ap = m._allPrototypes\n",
    "numIter = len(ap[0])\n",
    "\n",
    "amIters = [[] for _ in range(numIter)]\n",
    "for n in range(numIter):\n",
    "    for g in gestures:\n",
    "        amIters[n].append(ap[g][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'centroids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a874c0eed6ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentPrototypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprototypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prototypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'centroids' is not defined"
     ]
    }
   ],
   "source": [
    "_, centPrototypes = centroids(X,label=y)\n",
    "prototypes = m._prototypes\n",
    "\n",
    "f,ax = plt.subplots(1,2,figsize=(20,10))\n",
    "ax = ax.flatten()\n",
    "for p in positions:\n",
    "    analyze_prototypes(centPrototypes,hv[posLabel==p],gestLabel[posLabel==p],ax,isMain=False)\n",
    "analyze_prototypes(centPrototypes,hv,gestLabel,ax,isMain=True)\n",
    "ax[0].set_xlabel('Hamming distance to correct prototype')\n",
    "ax[1].set_xlabel('Hamming distance margin')\n",
    "ax[0].set_ylabel('Probability density')\n",
    "ax[1].set_ylabel('Probability density')\n",
    "    \n",
    "# analyze_prototypes(centPrototypes,hv,gestLabel,ax)\n",
    "# analyze_prototypes(prototypes,hv,gestLabel,ax)\n",
    "ax[0].set_xlim((0,0.55))\n",
    "ax[1].set_xlim((-0.25,0.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
