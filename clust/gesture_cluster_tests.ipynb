{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general purpose\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# calculations\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# clustering\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN, AffinityPropagation\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "Experiment 2 contains all 13 gestures in all 8 arm positions, 3 trials each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dataset and encoding type\n",
    "dataName = 'allHV.npz'\n",
    "emgHVType =  'hvRel'\n",
    "\n",
    "allHV = np.load(dataName)\n",
    "\n",
    "# extract data and labels based on gesture, trial, and position\n",
    "hv = allHV[emgHVType]\n",
    "gestLabel = allHV['gestLabel']\n",
    "posLabel = allHV['posLabel']\n",
    "trialLabel = allHV['trialLabel']\n",
    "\n",
    "# get list of unique values for each label\n",
    "gestures = np.unique(gestLabel)\n",
    "positions = np.unique(posLabel)\n",
    "trials = np.unique(trialLabel)\n",
    "\n",
    "numGestures = len(gestures)\n",
    "numPositions = len(positions)\n",
    "numTrials = len(trials)\n",
    "\n",
    "# get data size info\n",
    "D = hv.shape[1] # hypervector dimension\n",
    "numHV = 80 # number of examples per trial\n",
    "\n",
    "# color palettes for plotting\n",
    "gPalette = sns.color_palette('Paired', numGestures)\n",
    "pPalette = sns.color_palette('Paired', numPositions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load projection data for visualization\n",
    "Used UMAP for reduction down to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performed across entire dataset\n",
    "proj = np.load('./projections/projAll.npy')\n",
    "\n",
    "# performed separately for each gesture \n",
    "projG = []\n",
    "for g in gestures:\n",
    "    projG.append(np.load('./projections/projG' + str(g) + '.npy'))\n",
    "    \n",
    "# performed separately for each arm position\n",
    "projP = []\n",
    "for p in positions:\n",
    "    projP.append(np.load('./projections/projP' + str(p) + '.npy'))\n",
    "    \n",
    "# performed separately for each gesture/arm position pair\n",
    "projGP = []\n",
    "for g in gestures:\n",
    "    temp = []\n",
    "    for p in positions:\n",
    "        temp.append(np.load('./projections/projG' + str(g) + 'P' + str(p) + '.npy'))\n",
    "    projGP.append(temp)\n",
    "\n",
    "# performed separately for each trial    \n",
    "projGPT = []\n",
    "for g in gestures:\n",
    "    temp1 = []\n",
    "    for p in positions:\n",
    "        temp2 = []\n",
    "        for t in trials:\n",
    "            temp2.append(np.load('./projections/projG' + str(g) + 'P' + str(p) + 'T' + str(t) + '.npy'))\n",
    "        temp1.append(temp2)\n",
    "    projGPT.append(temp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General purpose HD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipolarize(Y):\n",
    "    X = np.copy(Y)\n",
    "    X[X > 0] = 1.0\n",
    "    X[X < 0] = -1.0\n",
    "    X[X == 0] = np.random.choice([-1.0, 1.0], size=len(X[X == 0]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroids(X,label=None,doMean=False):\n",
    "    if label is not None:\n",
    "        c = np.zeros((len(np.unique(label)), X.shape[1]))\n",
    "        for i,l in enumerate(np.unique(label)):\n",
    "            if doMean:\n",
    "                c[i,:] = np.mean(X[label==l],axis=0)\n",
    "            else:\n",
    "                c[i,:] = bipolarize(np.sum(X[label==l],axis=0))\n",
    "    else:\n",
    "        if doMean:\n",
    "            c = np.mean(X,axis=0).reshape(1,-1)\n",
    "        else:\n",
    "            c = bipolarize(np.sum(X,axis=0)).reshape(1,-1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_singleton_clusters(clusterLabels):\n",
    "    X = np.copy(clusterLabels)\n",
    "    clusts = np.unique(X)\n",
    "    for c in clusts:\n",
    "        if len(X[X == c]) == 1:\n",
    "            X[X == c] = -1.0\n",
    "    clusts = np.unique(X[X != -1])\n",
    "    for i,c in enumerate(clusts):\n",
    "        X[X == c] = i\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_outliers_singleton(clusterLabels):\n",
    "    X = np.copy(clusterLabels)\n",
    "    numOutliers = len(X[X == -1])\n",
    "    X[X == -1] = np.arange(numOutliers) + max(X) + 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering evaluation\n",
    "\n",
    "### cluster_metrics(X, label, classLabel)\n",
    "- Calculates both unsupervised and supervised clustering metrics on a particular clustering\n",
    "- Arguments:\n",
    "    - X: points being clustered\n",
    "    - label: clustering labels\n",
    "    - classLabel: supervised labels for comparison\n",
    "- Returns:\n",
    "    - out: dictionary containing all metrics, described below\n",
    "    \n",
    "### numClusts\n",
    "Number of clusters including singleton clusters and outliers\n",
    "\n",
    "### numOutliers\n",
    "Number of outliers/singleton clusters\n",
    "\n",
    "## Unsupervised metrics\n",
    "\n",
    "### I1 (minimize)\n",
    "Measure of overall graph-based cohesion. For each cluster, get average pairwise distance between points. Compute weighted sum over all clusters weighted by cluster size $m_i$.\n",
    "\\begin{equation*}\n",
    "I_1 = \\sum\\limits_{i}{\\frac{1}{m_i}\\sum\\limits_{x \\in C_i, y \\in C_i}{dist(x,y)}}\n",
    "\\end{equation*}\n",
    "\n",
    "### I2 (minimize)\n",
    "Measure of overall prototype-based cohesion. Sum the distances between each point and the centroid of the cluster it belongs to. \n",
    "\\begin{equation*}\n",
    "I_2 = \\sum\\limits_{i}{\\sum\\limits_{x \\in C_i}{dist(x,c_i)}}\n",
    "\\end{equation*}\n",
    "\n",
    "### E1 (maximize)\n",
    "Measure of prototype-based separation. Sum the distances between each cluster prototype and the overall data centroid, weighted by cluster size. \n",
    "\\begin{equation*}\n",
    "E_1 = \\sum\\limits_{i}{m_i dist(c_i,c_{all})}\n",
    "\\end{equation*}\n",
    "\n",
    "### G1 (maximize)\n",
    "Metric combining graph-based separation and cohesion. Sum the distances between points in a cluster and all other clusters, and divide by the within-cluster cohesion.\n",
    "\\begin{equation*}\n",
    "G_1 = \\sum\\limits_{i}{\\frac{\\sum\\limits_{j,j\\neq i}{\\sum\\limits_{x \\in C_i, y \\in C_j}{dist(x,y)}}}{\\sum\\limits_{x \\in C_i, y \\in C_i}{dist(x,y)}}}\n",
    "\\end{equation*}\n",
    "\n",
    "### silScore (maximize)\n",
    "Silhouette score, averaged across all objects. Effectively a normalized difference between distance to points within cluster and distance to points within closest separate cluster. Each object's silhouette score is computed as follows:\n",
    "1. Compute the average distance between it and all objects in the same cluster. Call this $a_i$.\n",
    "1. Compute the average distance between it and all objects in each other cluster. Get the minimum value and call this $b_i$.\n",
    "The silhouette score for object $x_i$ is then:\n",
    "\\begin{equation*}\n",
    "s_i = \\frac{b_i - a_i}{max(a_i, b_i)}\n",
    "\\end{equation*}\n",
    "\n",
    "### CHindex (maximize)\n",
    "Calinski-Harabasz Index, or Variance Ratio Criterion. Ratio of the sum of between-clusters dispersion and of inter-cluster dispersion, where dispersion is the sum of distances squared.\n",
    "\\begin{equation*}\n",
    "VRC = \\frac{SS_B}{SS_W} \\times \\frac{N-k}{k-1}\n",
    "\\end{equation*}\n",
    "where $SS_B$ is the between-cluster variance:\n",
    "\\begin{equation*}\n",
    "SS_B = \\sum\\limits_{i\\in k}{n_i ||m_i - m||^2}\n",
    "\\end{equation*}\n",
    "$SS_W$ is the within-cluster variance:\n",
    "\\begin{equation*}\n",
    "SS_W = \\sum\\limits_{i\\in k}{\\sum\\limits_{x \\in C_i}{||x - m_i||^2}}\n",
    "\\end{equation*}\n",
    "$N$ is overall number of objects, $k$ is number of classes, $n_i$ is the number of objects in each class, $m_i$ is centroid of each class, and $m$ is centroid of all data. This is very similar to G1, except based on squared Euclidean distance instead of Hamming distance.\n",
    "\n",
    "### DBscore (minimize)\n",
    "Davies-Bouldin Index. Signifies the average similarity between clusters, where similarity compares distance between clusters with the size of clusters themselves.\n",
    "\\begin{equation*}\n",
    "DB = \\frac{1}{k}\\sum\\limits_{i \\in k}{R_i} \\\\\n",
    "R_i = \\max\\limits_{j \\in k, j \\neq i}{R_{ij}} \\\\\n",
    "R_{ij} = \\frac{s_i + s_j}{d_{ij}} \\\\\n",
    "d_{ij} = dist(c_i, c_j) \\\\\n",
    "s_i = \\frac{1}{||c_i||}\\sum\\limits_{x \\in C_i}{dist(x,c_i)}\n",
    "\\end{equation*}\n",
    "\n",
    "## Supervised metrics\n",
    "\n",
    "### entropy (minimize)\n",
    "The degree to which each cluster consists of object of a single class. Based on clustering precision$p_{ij} = m_{ij}/m_j$ where number of objects $m$ in cluster $i$ and class $j$. Entropy for each cluster is calculated as $e_i = -\\sum\\limits_{j}{p_{ij}log_2p_{ij}}$\n",
    "\n",
    "### purity (maximize)\n",
    "Similar to entropy, where purity for each cluster is $p_i = \\max\\limits_{j}{p_{ij}}$\n",
    "\n",
    "### fMeas (maximize)\n",
    "F-measure, a combination of precision and recall, $r_{ij} = m_{ij}/m_j$. \n",
    "\\begin{equation*}\n",
    "F(i,j) = \\frac{2\\times p_{ij} \\times r_{ij}}{p_{ij} + r_{ij}}\n",
    "\\end{equation*}\n",
    "For overall score, take max for each cluster over classes, and sum weighted by cluster size.\n",
    "\n",
    "### adjRand (maximize)\n",
    "Rand index calculated using:\n",
    "- $a$: the number of pairs of elements that are in the same set in clustering and in the same set in the supervised labels\n",
    "- $b$: the number of pairs of elements that are in different sets in clustering and in different sets in the supervised labels\n",
    "\n",
    "The raw, unadjsted form is:$RI = \\frac{a+b}{C}$ where $C$ is the notal number of possible pairs. The adjusted version accounts for the score of random labelings $$ARI = \\frac{RI - E[RI]}{\\max(RI) - E[RI]}$$\n",
    "\n",
    "### adjMI (maximize)\n",
    "The level of mutual information shared between labeling and clustering. \n",
    "\n",
    "### homogeneity (maximize)\n",
    "Metric for degree to which each cluster contains only memebrs of a single class.\n",
    "\n",
    "### completeness (maximize)\n",
    "Metric for degree to which all members of a given class are assigned to the same cluster.\n",
    "\n",
    "### vMeas (maximize)\n",
    "Harmonic mean of homogeneity and completeness.\n",
    "\n",
    "### FMscore (maximize)\n",
    "Geometric mean of pairwise precision and recall: $$FMI = \\frac{TP}{\\sqrt{(TP + FP)(TP + FN)}}$$ where $TP$ is the true positive rate (number of pairs of points that belong to the same clusters in both sets of labels), $FP$ is the false positive rate (number of pairs of points that belong to the same clusters in true labels but not predicted labels), $FN$ is false negative (number of pairs of points that belongs in the same clusters in the predicted labels but not in true labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_metrics(X,label,classLabel):\n",
    "    \n",
    "    out = {} # create output dictionary\n",
    "    \n",
    "    # relabel all outliers so that they become singleton clusters\n",
    "    clustLabel = make_outliers_singleton(label)\n",
    "    \n",
    "    ## get unsupervised metrics\n",
    "    \n",
    "    clusts = np.unique(clustLabel) # first get a list of all unique cluster labels\n",
    "    numClusts = len(clusts) # calculate the total number clusters\n",
    "    outliers = [c for c in clusts if len(clustLabel[clustLabel==c]) == 1] # get all singleton clusters\n",
    "    \n",
    "    out['totalClusts'] = numClusts\n",
    "    out['numOutliers'] = len(outliers)\n",
    "    out['numClusts'] = numClusts - len(outliers)\n",
    "    \n",
    "    # first go through and calculate all of the within-cluster metrics\n",
    "    m = np.zeros(numClusts) # number of objects in each cluster\n",
    "    graphCo = np.zeros(numClusts) # graph-based cohesion\n",
    "    protoCo = np.zeros(numClusts) # prototype-based cohesion\n",
    "    protoSepSingle = np.zeros(numClusts) # prototype-based separation with centroid of entire set\n",
    "    allCentroid = bipolarize(np.sum(X,axis=0)).reshape(1,-1) # centroid of entire set\n",
    "    \n",
    "    for c in clusts:\n",
    "        m[c] = len(clustLabel[clustLabel==c])\n",
    "#         graphCo[c] = max(np.sum(pairwise_distances(X[clustLabel==c],metric='hamming')), 1/X.shape[0])\n",
    "        graphCo[c] = np.sum(pairwise_distances(X[clustLabel==c],metric='hamming'))\n",
    "#         if graphCo[c] == 0:\n",
    "#             graphCo[c] = X.shape[0]\n",
    "        proto = bipolarize(np.sum(X[clustLabel==c],axis=0)).reshape(1,-1)\n",
    "        protoCo[c] = np.sum(cdist(X[clustLabel==c],proto,'hamming'))\n",
    "        protoSepSingle[c] = np.sum(cdist(allCentroid,proto,'hamming'))\n",
    "    \n",
    "    # calculate the overall metrics\n",
    "    I1 = sum(graphCo/m)\n",
    "    I2 = sum(protoCo)\n",
    "    out['I1'] = I1\n",
    "    out['I2'] = I2\n",
    "    \n",
    "    graphCo[graphCo == 0] = np.Infinity\n",
    "    \n",
    "    # now get separation metrics between pairs of clusters\n",
    "    graphSep = np.zeros(len(list(combinations(clusts, 2)))) # graph-based separation\n",
    "    protoSepPair = np.zeros(len(list(combinations(clusts, 2)))) # prototype-based separation between pairs of centroids\n",
    "    graphSepSums = np.zeros(numClusts) # precalc for G1\n",
    "    \n",
    "    for c,comb in enumerate(combinations(clusts, 2)):\n",
    "        X0 = X[clustLabel==comb[0]]\n",
    "        X1 = X[clustLabel==comb[1]]\n",
    "        graphSep[c] = np.sum(cdist(X0,X1,'hamming'))\n",
    "        \n",
    "        proto0 = bipolarize(np.sum(X0,axis=0)).reshape(1,-1)\n",
    "        proto1 = bipolarize(np.sum(X1,axis=0)).reshape(1,-1)\n",
    "        protoSepPair[c] = np.sum(cdist(proto0,proto1,'hamming'))\n",
    "        \n",
    "        graphSepSums[comb[0]] += graphSep[c]\n",
    "        graphSepSums[comb[1]] += graphSep[c]\n",
    "    \n",
    "    # calcualte overall metrics\n",
    "    E1 = sum(protoSepSingle*m)\n",
    "    G1 = sum(graphSepSums/graphCo)\n",
    "    \n",
    "    out['E1'] = E1\n",
    "    out['G1'] = G1\n",
    "    \n",
    "    # next compute holistic scores\n",
    "    out['silScore'] = metrics.silhouette_score(X,clustLabel,'hamming')\n",
    "    out['CHindex'] = metrics.calinski_harabasz_score(X, clustLabel)\n",
    "    out['DBscore'] = metrics.davies_bouldin_score(X, clustLabel)\n",
    "    \n",
    "    # get supervised metrics\n",
    "    \n",
    "    classes = np.unique(classLabel) # next get a list of all unique class labels\n",
    "    numClasses = len(classes) # number of actual classes\n",
    "    \n",
    "    e_i = np.zeros(numClusts)\n",
    "    m_i = np.zeros(numClusts)\n",
    "    p_i = np.zeros(numClusts)\n",
    "    f_ij = np.zeros((numClusts,numClasses))\n",
    "    m_j = np.zeros(numClasses)\n",
    "    for i in clusts:\n",
    "        m_i[i] = len(clustLabel[clustLabel==i])\n",
    "        for j in classes:\n",
    "            m_ij = len(clustLabel[(clustLabel==i) & (classLabel==j)])\n",
    "            m_j[j] = len(clustLabel[classLabel==j])\n",
    "            r_ij = m_ij/m_j[j]\n",
    "            p_ij = m_ij/m_i[i]\n",
    "            if m_ij > 0:\n",
    "                f_ij[i,j] = 2*p_ij*r_ij/(p_ij + r_ij)\n",
    "                e_i[i] += -p_ij*np.log2(p_ij)\n",
    "                p_i[i] = max(p_ij,p_i[i])\n",
    "                \n",
    "    entropy = sum(e_i*m_i)/len(clustLabel)\n",
    "    purity = sum(p_i*m_i)/len(clustLabel)\n",
    "    fMeas = sum(np.max(f_ij,axis=0)*m_j)/len(clustLabel)\n",
    "    \n",
    "    out['entropy'] = entropy\n",
    "    out['purity'] = purity\n",
    "    out['fMeas'] = fMeas\n",
    "    \n",
    "    out['adjRand'] = metrics.adjusted_rand_score(clustLabel,classLabel)\n",
    "    out['adjMI'] = metrics.adjusted_mutual_info_score(clustLabel,classLabel)\n",
    "    out['homogeneity'] = metrics.homogeneity_score(clustLabel,classLabel)\n",
    "    out['completeness'] = metrics.completeness_score(clustLabel,classLabel)\n",
    "    out['vMeas'] = metrics.v_measure_score(clustLabel,classLabel)\n",
    "    out['FMscore'] = metrics.fowlkes_mallows_score(clustLabel,classLabel)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_metrics_1d(res,x,xName,clustName):\n",
    "    \n",
    "    met = {}\n",
    "    met['totalClusts'] = ('Total number of clusters',min)\n",
    "    met['numClusts'] = ('Number of non-singleton clusters',min)\n",
    "    met['numOutliers'] = ('Number of outliers/singleton clusters',min)\n",
    "    met['I1'] = ('Graph-based cohesion',min)\n",
    "    met['I2'] = ('Prototype-based cohesion',min)\n",
    "    met['E1'] = ('Prototype-based separation',max)\n",
    "    met['G1'] = ('Graph-based separation/cohesion',max)\n",
    "    met['silScore'] = ('Silhouette score',max)\n",
    "    met['CHindex'] = ('Calinsky-Harabasz score',max)\n",
    "    met['DBscore'] = ('Davies-Bouldin score',min)\n",
    "    met['entropy'] = ('Entropy',min)\n",
    "    met['purity'] = ('Purity',max)\n",
    "    met['fMeas'] = ('F-Measure',max)\n",
    "    met['adjRand'] = ('Adjusted Rand index',max)\n",
    "    met['adjMI'] = ('Adjusted mutual information',max)\n",
    "    met['homogeneity'] = ('Homogeneity',max)\n",
    "    met['completeness'] = ('Completeness',max)\n",
    "    met['vMeas'] = ('V-Measure',max)\n",
    "    met['FMscore'] = ('Fowlkes-Mallows score',max)\n",
    "    \n",
    "    f,axes = plt.subplots(int(np.ceil(len(met.keys())/4)),4,figsize=(30,int(np.ceil(len(met.keys())/4))*6 + 4))\n",
    "    f.subplots_adjust(hspace=0.3,wspace=0.3)\n",
    "\n",
    "    f.suptitle('Cluster metrics - ' + clustName)\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    for i,k in enumerate(met.keys()):\n",
    "        axes[i].plot(x,res[k])\n",
    "        axes[i].set(title=met[k][0],xlabel=xName,ylabel=k)\n",
    "        \n",
    "        extVal = met[k][1](res[k])\n",
    "        extX = np.array(x)[(np.array(res[k]) == extVal)]\n",
    "        extY = extVal*np.ones(len(extX))\n",
    "        axes[i].scatter(extX,extY,marker='*',c='m',s=400)\n",
    "\n",
    "    for ax in axes[len(met.keys())::]:\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_hdc(Y,t,perm=None):\n",
    "    label = -np.ones(Y.shape[0],dtype='int32')\n",
    "    clusts = []\n",
    "    \n",
    "    if perm is not None:\n",
    "        X = np.copy(Y[perm])\n",
    "    else:\n",
    "        X = np.copy(Y)\n",
    "    \n",
    "    for i,x in enumerate(X):\n",
    "        if not clusts:\n",
    "            clusts.append(0)\n",
    "            label[i] = 0\n",
    "        else:\n",
    "            sim = np.zeros(len(clusts))\n",
    "            for c in clusts:\n",
    "                proto = bipolarize(np.sum(X[label==c],axis=0)).reshape(1,-1)\n",
    "                sim[c] = 1 - cdist(proto,x.reshape(1,-1),'hamming')\n",
    "            if np.max(sim) > t:\n",
    "                label[i] = np.argmax(sim)\n",
    "            else:\n",
    "                label[i] = max(clusts) + 1\n",
    "                clusts.append(max(clusts) + 1)\n",
    "                \n",
    "    if perm is not None:\n",
    "        return label[np.argsort(perm)]\n",
    "    else:\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_affinity(X,prefScale,damping):\n",
    "    affinity = -pairwise_distances(X,metric='hamming')\n",
    "    pref = prefScale*(np.min(affinity) - np.median(affinity)) + np.median(affinity)\n",
    "    cl = AffinityPropagation(damping=damping,\n",
    "                             max_iter=500,\n",
    "                             convergence_iter=15,\n",
    "                             copy=True,\n",
    "                             preference=pref,\n",
    "                             affinity='precomputed',\n",
    "                             verbose=0).fit(affinity)\n",
    "    return cl.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_kmeans_noinit(X,n_clusters):\n",
    "    cl = KMeans(n_clusters=n_clusters,\n",
    "                init='k-means++',\n",
    "                n_init=100,\n",
    "                max_iter=1000,\n",
    "                tol = 1e-6,\n",
    "                precompute_distances='auto',\n",
    "                verbose=0,\n",
    "                random_state=None,\n",
    "                copy_x=True,\n",
    "                n_jobs=-1,\n",
    "                algorithm='auto').fit(X)\n",
    "    return cl.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_kmeans_init(X,n_clusters,init_clusters):\n",
    "    cl = KMeans(n_clusters=n_clusters,\n",
    "                init=init_clusters,\n",
    "                n_init=1,\n",
    "                max_iter=1000,\n",
    "                tol = 1e-6,\n",
    "                precompute_distances='auto',\n",
    "                verbose=0,\n",
    "                random_state=None,\n",
    "                copy_x=True,\n",
    "                n_jobs=-1,\n",
    "                algorithm='auto').fit(X)\n",
    "    return cl.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_hdbscan(X,minClust,minSamp):\n",
    "    cl = hdbscan.HDBSCAN(min_cluster_size=minClust,\n",
    "                         min_samples=minSamp,\n",
    "                         metric='hamming',\n",
    "                         p=None,\n",
    "                         alpha=None,\n",
    "                         cluster_selection_epsilon=0.0,\n",
    "                         algorithm='best',\n",
    "                         leaf_size=40,\n",
    "                         approx_min_span_tree=True,\n",
    "                         gen_min_span_tree=False,\n",
    "                         core_dist_n_jobs=-1,\n",
    "                         cluster_selection_method='eom',\n",
    "                         allow_single_cluster=False,\n",
    "                         prediction_data=False,\n",
    "                         match_reference_implementation=False).fit(X)\n",
    "    softClusters = hdbscan.all_points_membership_vectors(cl)\n",
    "    return np.array([np.argmax(x) for x in softClusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_dbscan(X,eps,minSamp):\n",
    "    cl = DBSCAN(eps=eps,\n",
    "                min_samples=minSamp,\n",
    "                metric='hamming',\n",
    "                metric_params=None,\n",
    "                algorithm='auto',\n",
    "                leaf_size=30,\n",
    "                p=None,\n",
    "                n_jobs=-1).fit(X)\n",
    "    return cl.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_hdc(X,verbose=False, random=False):\n",
    "    \n",
    "    numIter = 100\n",
    "    labels = {}\n",
    "    \n",
    "    tUpper = 1\n",
    "    \n",
    "    tMax = 1\n",
    "    tMin = 0.5\n",
    "    t = tMin\n",
    "    n = 0\n",
    "    \n",
    "    lowestClust = X.shape[0] + 1\n",
    "    if random:\n",
    "        for i in range(numIter):\n",
    "            if verbose:\n",
    "                print(\"\\rRuning random permutation %d out of %d\" % (i+1, numIter), end =\" \") \n",
    "            temp = clust_hdc(X,t,np.random.permutation(X.shape[0]))\n",
    "            if len(np.unique(temp)) < lowestClust:\n",
    "                lab = temp\n",
    "    else:\n",
    "        lab = clust_hdc(X,t)\n",
    "    \n",
    "    labels[t] = lab\n",
    "\n",
    "    clusts = np.unique(lab)\n",
    "    numClust = len(clusts)\n",
    "    outliers = [c for c in clusts if len(lab[lab==c]) == 1]\n",
    "    numNonSingle = numClust - len(outliers)\n",
    "    \n",
    "    if verbose:\n",
    "        if random:\n",
    "            print(', threshold = %f (iteration %d): %d total clusters, %d non-singleton' % (t, n, numClust, numNonSingle))\n",
    "        else:\n",
    "            print('Iteration %d, threshold = %f: %d total clusters, %d non-singleton' % (n, t, numClust, numNonSingle))\n",
    "\n",
    "    while ((numNonSingle != 2) or (numClust > round(X.shape[0]/2))) and (n < 10):\n",
    "        n += 1\n",
    "        if numNonSingle == 1:\n",
    "            tMin = t\n",
    "            t = np.mean([t,tMax])\n",
    "        else:\n",
    "            tMax = t\n",
    "            t = np.mean([t,tMin])\n",
    "            \n",
    "        lowestClust = X.shape[0] + 1\n",
    "        if random:\n",
    "            if verbose:\n",
    "                print(\"\\rRuning random permutation %d out of %d\" % (i+1, numIter), end =\" \") \n",
    "            for i in range(numIter):\n",
    "                temp = clust_hdc(X,t,np.random.permutation(X.shape[0]))\n",
    "                if len(np.unique(temp)) < lowestClust:\n",
    "                    lab = temp\n",
    "        else:\n",
    "            lab = clust_hdc(X,t)\n",
    "        \n",
    "        labels[t] = lab\n",
    "\n",
    "        clusts = np.unique(lab)\n",
    "        numClust = len(clusts)\n",
    "        outliers = [c for c in clusts if len(lab[lab==c]) == 1]\n",
    "        numNonSingle = numClust - len(outliers)\n",
    "        \n",
    "        if verbose:\n",
    "            if random:\n",
    "                print(', threshold = %f (iteration %d): %d total clusters, %d non-singleton' % (t, n, numClust, numNonSingle))\n",
    "            else:\n",
    "                print('Iteration %d, threshold = %f: %d total clusters, %d non-singleton' % (n, t, numClust, numNonSingle))\n",
    "            \n",
    "        if numClust > X.shape[0]/10:\n",
    "            tUpper = min(tUpper, t)\n",
    "            \n",
    "    N = 20\n",
    "    for t in np.linspace(t + (tUpper-t)/(N+1),tUpper,N,endpoint=False):\n",
    "        n += 1\n",
    "        lowestClust = X.shape[0] + 1\n",
    "        if random:\n",
    "            for i in range(numIter):\n",
    "                if verbose:\n",
    "                    print(\"\\rRuning random permutation %d out of %d\" % (i+1, numIter), end =\" \") \n",
    "                temp = clust_hdc(X,t,np.random.permutation(X.shape[0]))\n",
    "                if len(np.unique(temp)) < lowestClust:\n",
    "                    lab = temp\n",
    "        else:\n",
    "            lab = clust_hdc(X,t)\n",
    "        \n",
    "        labels[t] = lab\n",
    "\n",
    "        clusts = np.unique(lab)\n",
    "        numClust = len(clusts)\n",
    "        outliers = [c for c in clusts if len(lab[lab==c]) == 1]\n",
    "        numNonSingle = numClust - len(outliers)\n",
    "        \n",
    "        if verbose:\n",
    "            if random:\n",
    "                print(', threshold = %f (iteration %d): %d total clusters, %d non-singleton' % (t, n, numClust, numNonSingle))\n",
    "            else:\n",
    "                print('Iteration %d, threshold = %f: %d total clusters, %d non-singleton' % (n, t, numClust, numNonSingle))\n",
    "        \n",
    "        if numClust > len(X)/10:\n",
    "            break\n",
    "            \n",
    "    return labels\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_affinity_prop(X,verbose=False):\n",
    "    \n",
    "    labels = {}\n",
    "    \n",
    "    for d in [0.5, 0.7, 0.9, 0.95, 0.97, 0.98, 0.99]:\n",
    "        currPref = 0\n",
    "        n = 0\n",
    "        lab = clust_affinity(X,currPref,d)\n",
    "        labels[(d,currPref)] = lab\n",
    "\n",
    "        clusts = np.unique(lab)\n",
    "        numClust = len(clusts)\n",
    "        outliers = [c for c in clusts if len(lab[lab==c]) == 1]\n",
    "        numNonSingle = numClust - len(outliers)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Iteration %d, preference scale = %d, damping = %f: %d total clusters, %d non-singleton' % (n, currPref, d, numClust, numNonSingle))\n",
    "\n",
    "        currPref = 1\n",
    "        n = 1\n",
    "        lab = clust_affinity(X,currPref,d)\n",
    "        labels[(d,currPref)] = lab\n",
    "\n",
    "        clusts = np.unique(lab)\n",
    "        numClust = len(clusts)\n",
    "        outliers = [c for c in clusts if len(lab[lab==c]) == 1]\n",
    "        numNonSingle = numClust - len(outliers)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Iteration %d, preference scale = %d, damping = %f: %d total clusters, %d non-singleton' % (n, currPref, d, numClust, numNonSingle))\n",
    "\n",
    "        while numNonSingle > 1:\n",
    "            n += 1\n",
    "            currPref = currPref*2\n",
    "            lab = clust_affinity(X,currPref,d)\n",
    "            labels[(d,currPref)] = lab\n",
    "\n",
    "            clusts = np.unique(lab)\n",
    "            numClust = len(clusts)\n",
    "            outliers = [c for c in clusts if len(lab[lab==c]) == 1]\n",
    "            numNonSingle = numClust - len(outliers)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Iteration %d, preference scale = %d, damping = %f: %d total clusters, %d non-singleton' % (n, currPref, d, numClust, numNonSingle))\n",
    "\n",
    "        step = currPref/4\n",
    "        while step >= 0.1:\n",
    "            if numNonSingle == 1:\n",
    "                currPref -= step\n",
    "            else:\n",
    "                currPref += step\n",
    "            n += 1\n",
    "\n",
    "            lab = clust_affinity(X,currPref,d)\n",
    "            labels[(d,currPref)] = lab  \n",
    "\n",
    "            clusts = np.unique(lab)\n",
    "            numClust = len(clusts)\n",
    "            outliers = [c for c in clusts if len(lab[lab==c]) == 1]\n",
    "            numNonSingle = numClust - len(outliers)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Iteration %d, preference scale = %f, damping = %f: %d total clusters, %d non-singleton' % (n, currPref, d, numClust, numNonSingle))\n",
    "\n",
    "            step /= 2\n",
    "            \n",
    "    return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_agglomerative(X,method,metric):\n",
    "    if metric == 'hamming':\n",
    "        Z = linkage(pdist(X,metric='hamming'),method=method,metric=metric)\n",
    "    else:\n",
    "        Z = linkage(X,method=method,metric=metric)\n",
    "    lab = {}\n",
    "    for n in range(len(X)):\n",
    "        lab[n+1] = fcluster(Z,n+1,criterion='maxclust') - 1\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_kmeans(X,init_labels=None,verbose=False):\n",
    "    maxClust = round(len(X)/10)\n",
    "    labels = {}\n",
    "    if init_labels is not None:\n",
    "        for n in range(1,maxClust+1):\n",
    "            ic = centroids(X,label=init_labels[n])\n",
    "            labels[n] = clust_kmeans_init(X,n,ic)\n",
    "            if verbose:\n",
    "                lab = labels[n]\n",
    "                clusts = np.unique(lab)\n",
    "                numClust = len(clusts)\n",
    "                outliers = [c for c in clusts if len(lab[lab==c]) == 1]\n",
    "                numNonSingle = numClust - len(outliers)\n",
    "                print('Running k-means with k = %d: got %d non-single clusters' % (n, numNonSingle))\n",
    "    else:\n",
    "        for n in range(1,maxClust+1):\n",
    "            labels[n] = clust_kmeans_noinit(X,n)\n",
    "            if verbose:\n",
    "                lab = labels[n]\n",
    "                clusts = np.unique(lab)\n",
    "                numClust = len(clusts)\n",
    "                outliers = [c for c in clusts if len(lab[lab==c]) == 1]\n",
    "                numNonSingle = numClust - len(outliers)\n",
    "                print('Running k-means with k = %d: got %d non-single clusters' % (n, numNonSingle))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0\n",
    "filt = gestLabel == g\n",
    "X = hv[filt]\n",
    "\n",
    "# l = labels_affinity_prop(X,verbose=True)\n",
    "# with open('./clusters/labels_affinity_prop.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# l = labels_hdc(X,verbose=True,random=False)\n",
    "# with open('./clusters/labels_hdc.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# l = labels_hdc(X,verbose=True,random=True)\n",
    "# with open('./clusters/labels_hdc_best.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# l = labels_agglomerative(X,'ward','euclidean')\n",
    "# with open('./clusters/labels_agglomerative_w_e.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# l = labels_agglomerative(X,'centroid','euclidean')\n",
    "# with open('./clusters/labels_agglomerative_c_e.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# l = labels_agglomerative(X,'median','euclidean')\n",
    "# with open('./clusters/labels_agglomerative_m_e.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# l = labels_agglomerative(X,'ward','hamming')\n",
    "# with open('./clusters/labels_agglomerative_w_h.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# l = labels_agglomerative(X,'centroid','hamming')\n",
    "# with open('./clusters/labels_agglomerative_c_h.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# l = labels_agglomerative(X,'median','hamming')\n",
    "# with open('./clusters/labels_agglomerative_m_h.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# l = labels_kmeans(X,verbose=True)\n",
    "# with open('./clusters/labels_kmeans.pickle','wb') as f:\n",
    "#     pickle.dump(l,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how to get labels from combinations of gestures/positions/trials\n",
    "# trialSubset = trialLabel[filt]\n",
    "# gestSubset = gestLabel[filt]\n",
    "# posSubset = posLabel[filt]\n",
    "\n",
    "# combs, superLabels = np.unique(np.column_stack((trialSubset,gestSubset,posSubset)),axis=0,return_inverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
